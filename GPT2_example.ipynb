{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c891c69-add3-42ca-927c-f5bd171e8c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0056, 0.3390, 0.9256],\n",
      "        [0.9903, 0.9105, 0.6547],\n",
      "        [0.8607, 0.7236, 0.3772],\n",
      "        [0.8445, 0.1766, 0.6035],\n",
      "        [0.9281, 0.8128, 0.1723]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc93212-f27f-46ca-a842-df51ce425a49",
   "metadata": {},
   "source": [
    "### We apply OpenAI's GPT-2 model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cfbdc2-b31d-4053-96f4-7bc3760b93fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf8385327424a89bb2abbc2eef4f591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590f4e1982114b8c8c0853f48f7a4d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3451453a4bbe4dc6aeb930ecf3b566bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725da3dd678646638ec8ba42cacef04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17153583eb64e55b10ba4a8f7974a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e228a94-8a46-4a99-bdb7-507fbded7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3af1f1-9cc5-4c36-95b2-d8d30b8d6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e8d6b8-4999-4a89-a94d-a104f85b2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sequence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ef9ac3-59c7-4617-9e40-8d4b3cc50257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a07a61-fce0-44f2-904d-76c89aa99cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.\n",
      "\n",
      "In that regard, it is worth considering whether the model itself is an adequate tool in designing or building an advanced application of the principles involved. In the early 1980s, Dr. James Hansen used the work of the Center for Sustainability and the World Bank to demonstrate the importance of a model that would \"enhance the ability of scientists to develop and maintain reliable scientific data in diverse environments, from the remote areas of Africa to the tropics.\" While some authors suggested that he put the idea to theoretical physics, the fact that the model could help lay the foundation for the advanced future of predictive medicine is clear enough as Dr. Hansen has\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37151e35-7cc0-41d7-9ba9-41c69ae75fab",
   "metadata": {},
   "source": [
    "#### Another run but with higher __temperature__ makes the result a little more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df25b59c-4b0b-49e3-baf0-b1efb8acd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=200, do_sample=True, temperature=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30219bc-0505-4774-a947-5a0867edd9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.\n",
      "\n",
      "You'll know for a second there are important benefits to using the neural signal space—especially those that have the potential not seen until recently as a source of computational gains—if in our examples you simply add models to see its predictions. Because our data were created with this network to train our models in the initial stages, using this model as both an observer to get model results and more like our own as a guide, that also works really well.\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5a4f7-2032-47e9-9124-f3467bbabdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c891c69-add3-42ca-927c-f5bd171e8c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8574, 0.6569, 0.9673],\n",
      "        [0.6779, 0.2992, 0.7274],\n",
      "        [0.4033, 0.3956, 0.6735],\n",
      "        [0.5104, 0.6542, 0.0403],\n",
      "        [0.5595, 0.8972, 0.2993],\n",
      "        [0.3134, 0.7282, 0.4977],\n",
      "        [0.6491, 0.9446, 0.5919],\n",
      "        [0.9116, 0.6430, 0.5795]])\n"
     ]
    }
   ],
   "source": [
    "# Test Torch is working\n",
    "import torch\n",
    "x = torch.rand(8, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc93212-f27f-46ca-a842-df51ce425a49",
   "metadata": {},
   "source": [
    "### We apply OpenAI's GPT-2 model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cfbdc2-b31d-4053-96f4-7bc3760b93fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf8385327424a89bb2abbc2eef4f591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590f4e1982114b8c8c0853f48f7a4d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3451453a4bbe4dc6aeb930ecf3b566bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725da3dd678646638ec8ba42cacef04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17153583eb64e55b10ba4a8f7974a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e228a94-8a46-4a99-bdb7-507fbded7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3af1f1-9cc5-4c36-95b2-d8d30b8d6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e8d6b8-4999-4a89-a94d-a104f85b2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sequence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ef9ac3-59c7-4617-9e40-8d4b3cc50257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a07a61-fce0-44f2-904d-76c89aa99cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.\n",
      "\n",
      "In that regard, it is worth considering whether the model itself is an adequate tool in designing or building an advanced application of the principles involved. In the early 1980s, Dr. James Hansen used the work of the Center for Sustainability and the World Bank to demonstrate the importance of a model that would \"enhance the ability of scientists to develop and maintain reliable scientific data in diverse environments, from the remote areas of Africa to the tropics.\" While some authors suggested that he put the idea to theoretical physics, the fact that the model could help lay the foundation for the advanced future of predictive medicine is clear enough as Dr. Hansen has\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37151e35-7cc0-41d7-9ba9-41c69ae75fab",
   "metadata": {},
   "source": [
    "#### Another run but with higher __temperature__ makes the result a little more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df25b59c-4b0b-49e3-baf0-b1efb8acd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Use temperatur for a more general text \n",
    "outputs = model.generate(inputs, max_length=600, do_sample=True, temperature=1.53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30219bc-0505-4774-a947-5a0867edd9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples. Even if you can avoid underfitting without going to far as building the models yourself (or your clients at your local gym or your doctor), overfitting creates a risk of self instability, which results in overloading your brain, so there will tend to be greater instability of your relationships with your friends or family, but underlining your model results in less than optimal social interaction, even through your natural relationships! In fact, people who study with overfits should take these training sessions as an instruction on how to reduce overfitting while avoiding such results once they feel fine once more.\n",
      "\n",
      "We've previously stated why many overpriced approaches fall short: we try our best in building models ourselves; we take responsibility and make it happen ourselves with our models so well suited to the needs of our client base or team. However, over-fitting leads to stress—you should constantly adjust how you practice the way you practice. We would strongly urge that when dealing with overspending and learning too little, ask other experts from your firm before you go too far at you because as we discussed earlier that is important to train well at your firm and get back to good things if you take it that far. It could take several more workshops and hours, but it will make an effective training, which is key.\" (David Green).\n",
      "\n",
      "Here's what many of our professional and community clients want to tell us and more…\n",
      "\n",
      "\"I'd like to thank your network. Your training will make you whole, and as people know we'd very much like to work in a lab for over a third of their time.\n",
      "\n",
      "In your advice there could've been things that could have been fixed (i.e., changes to your existing job requirements if you do anything in the past 6 weeks) or perhaps had something happened (a situation in which a manager would've stopped paying your monthly rent and you've now paid more taxes). That advice comes with an asterisk which could be missing! That would've meant your team had more focus during these workouts and more communication before every challenge. Some of you would feel that you missed the mark even if you'd worked this very program.\" (Nadley Miller, founder)\n",
      "\n",
      "We all need people that build things and can work with the problem in front of us; but for the best solutions and most helpful ways for someone to build things, it won't be long with your service and mentoring. Please leave us any comment in support over at our Social Services Helpline or connect with David. If you are new to this industry and have something we'd like you to share here go out\n"
     ]
    }
   ],
   "source": [
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5a4f7-2032-47e9-9124-f3467bbabdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
